<!doctype html>
<html>
	<head>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

		<title>Serialization Protocols</title>

		<link rel="stylesheet" href="css/reveal.css">
		<link rel="stylesheet" href="css/theme/solarized.css">

		<!-- Theme used for syntax highlighting of code -->
		<link rel="stylesheet" href="lib/css/zenburn.css">

		<!-- Printing and PDF exports -->
		<script>
			var link = document.createElement( 'link' );
			link.rel = 'stylesheet';
			link.type = 'text/css';
			link.href = window.location.search.match( /print-pdf/gi ) ? 'css/print/pdf.css' : 'css/print/paper.css';
			document.getElementsByTagName( 'head' )[0].appendChild( link );
		</script>
	</head>
	<body>
		<div class="reveal">
			<div class="slides">


<!-- horrible indentation for a reason, because I don't want to read everything nested to far. -->

<section data-markdown data-separator="^\n---\n$" data-separator-vertical="^\n--\n$">
    <script type="text/template">
        # Serialization Protocols in Scala
        a shootout

        ---

        ## Reasoning
        so why is this even important?


        --

        ## System designs are changing

        1. Microservice architectures are becoming mainstream
        1. Queues (and distributed commit logs used as queues) show up in mainstream architectures
        1. CQRS / ES is a common design nowadays, especially with the rising popularity of DDD
        1. We're not using databases as integration layer anymore. (I hope?!...)

        --

        > Distributed systems are about protocols, not implementations. Forget languages, protocols are everything.

        [@TimPerret](https://twitter.com/timperrett/status/748590102926884864)


        --

        ### Evolvability

        > The ability of a system and its data to adopt to change

        --

        ### Evolvability

        In monolithic systems with relational databases, a change in the data model was usually implemented by 

        * a schema migration 
        * code changes
        * one big deployment

        --

		#### Evolvability Examples

        * monolithic systems and relational databases
        * replicated systems
        * distributed systems in your Datacenter 
        * systems with code distributed to the User

        --

        ### New Persistence Requirements

        * on-disk formats behave significantly different than in-memory formats

        * Data needs to be encoded/serialized/marshalled

        * to get it back you need to decode/deserialize/unmarshall

        --

        ### So how do can we deal with this?

    </script>
</section>
<section data-markdown data-separator="^\n---\n$" data-separator-vertical="^\n--\n$">
    <script type="text/template">
        ## Usual suspects

        --

        ### Programming Language provided serialization

        we get `scala.Serializable` for free, so why not just use it?

        --

        ### NoooooooooOOoo

        * We are bound to one implementation language (but we're Java Compatible)
        * The performance is awful
        * It has some serious drawbacks:

        --

        > Implement `Serializable` judiciously
        
        [Joshua Bloch, Effective Java]

        * decreases the flexibility to change the class once it is released
        * The inner implementation of your class becomes the API
        * increases the likelihood of bugs and security holes (Stack Overflows)
        * increases the testing burden

        --

        ### Some notes about Performance

        |protocol| create |ser|deser|total|size|
        |--|--|--|--|--|--|
        |protobuf|121|1173|719|1891|239|
        |thrift|95|1455|731|2186|349|
        |kryo-flat|55|705|909|1614|268|
        |avro-generic|329|1721|984|2704|221|
        |scala/java-built-in|514|8280|36105|44385|1293|

        [Source: jvm-serializers wiki](https://github.com/eishay/jvm-serializers/wiki)

        --

        ### Some notes about performance

        * as long as its one of our four, it's probably fast enough
        * if you need it faster, you'll have to do your own research
        * just don't use the built in serializer

        --

        ### But... how about JSON? Or XML?

        * widespread
        * somewhat human readable
        * still slow (depending on the benchmark)
        * verbose
        * number handling is difficult
        * character encoding is difficult

        --

        ### But... how about JSON? Or XML?

        There also exists a various range of schema extensions for JSON, like BSON and JSON Schema.
        Unfortunately they don't provide schema evolvability, so I'll skip them


    </script>
</section>
<section data-markdown data-separator="^\n---\n$" data-separator-vertical="^\n--\n$">
    <script type="text/template">
        ## Schema Evolvability

        --

        ### Reading Data between Schemas

        <img src="/media/broken_deployments.svg"  height="300" width="700">


        --

        ### Backwards compatibility

        This means a newer Schema can read Data encoded with an older Schema.

        That's halfway (not) easy. You know how the previous schema was, you can deal with missing things in your code. If you are aware of that.

        --

        ### Forwards compatibility

        Reading Data from a Schema that is newer that the Schema of the running Software.

        Ouch. How do you deal with things you don't know about?

        --

        ### Forwards compatibility

        Let a Protocol handle this for us!

        
    </script>
</section>
<section data-markdown data-separator="^\n---\n$" data-separator-vertical="^\n--\n$">
    <script type="text/template">
        ## Protobuf

        --

        What is Protbuf (protocol buffers)?

        > Protocol buffers are Google's language-neutral, platform-neutral, extensible mechanism for serializing structured data â€“ think XML, but smaller, faster, and simpler. 

        Besides the specification, Google provides a compiler for Java

        --

        ## ScalaPB

        ScalaPB is a protocol buffer compiler `(protoc)` plugin for Scala. It will generate Scala case classes, parsers and serializers for your protocol buffers

        --

        ## Schema Sample

        ```
        syntax = "proto3";

        package de.christianuhl.proto;

        message Person {
            int32 user_id = 1;
            string firstname = 2;
            string lastname = 3;
        }
        ```

        --

        ### Usage with SBT

        (live Sample)

        --

        ### Let's check the encoding on Disk

        |field tag| type |value |length |bytes / value |
        |--|--|--|--|--|--|
        |00010|010|12|09|43687269 73746961 6E|
        |00011|010|1A|03|0355686C|

        --

        ### Proto 2 vs Proto 3

        Significant changes:

        * No presence logic anymore
        * All values are optional by default, `required` semantic is removed
        * `Map` Type
        * Timevalues

        --

        ### Why the radical 'everything is optional'?   

        * `required` breaks schema evolvability anyways
        * loads of production issues
        * was already banned in Google
        * `optional` became redundant and was removed as well

    </script>
</section>

<section data-markdown data-separator="^\n---\n$" data-separator-vertical="^\n--\n$">
    <script type="text/template">
        ## Thrift

        --

        ### What is Thrift?

        > ... is an interface definition language and binary communication protocol that is used to define and create services for numerous languages. It is used as a remote procedure call (RPC) framework 

        [wikipedia](https://en.wikipedia.org/wiki/Apache_Thrift)

        --

        ### What is Thrift?

        * it has large conceptual similarities to protobuf
        * originally developed at Facebook
        * re-open-sourced later as fbthrift

        --

        ## Scala & SBT Integration

        Twitter supplies scrooge

        [Scrooge](https://github.com/twitter/scrooge)

        --

        ## Schema Sample

        ```
        struct Person {
            1: optional i64     userId,
            2: required string  firstname,
            3: required string  lastname
        }
        ```

        --

        ### Usage with SBT

        (live Sample)

        --

         ### Let's check the encoding on Disk

        |Type| Field Tag |Length |bytes | value |
        |--|--|--|--|--|
        |0B|0002|00000009|43687269 73746961 6E|Christian|
        |0B|0003|00000003|55686C|Uhl|

        --

        ### What makes Thrift special?

        * It's more than just serialization, it's a whole RPC framework

        * It embraces a whole family of encodings (They call it protocols)

        * Scala ecosystem only gets contributions by Twitter

        * otherwise, it's remarkably similar to protobuf

    </script>
</section>

<section data-markdown data-separator="^\n---\n$" data-separator-vertical="^\n--\n$">
    <script type="text/template">
        ## Avro

        --

        * started as a subproject of Hadoop, because Thrift was a bad fit for Hadoop

        * now an Apache project, supported by Confluent

        * Kafka (Confluent) is a large influencer nowadays

        * has two schema languages, one for humans and one for machines

        * direct mapping from and to JSON

        --

        differs from Protobuf and Thrift:

        * Reader and Writer Schema can be different, Data brings its own schema

        * less type information for the payload needed

        * no manually assigned field ids


        ### All boils down to having different schemas for reading and writing

        --

        ## Schema Sample

        ```

        record Person {
            union {null, long} user_id = null;
            string firstname;
            string lastname;
        }
        ```

        --

        ### Usage with SBT

        using avrohugger and avro4s

        * [avro4s](https://github.com/sksamuel/avro4s)
        * [avrohugger](https://github.com/julianpeeters/avrohugger)

        (live Sample)

        --

        ### Let's check the encoding on Disk


        first, 248 Bytes of schema

        |lenght| sing |value |bytes | value |
        |--|--|--|--|--|
        |0001001|0|12|43687269 73746961 6E|Christian|
        |0000011|0|06|55686C|Uhl|

        --

        ### So how do writer's and reader's schema go together?

        * they need to be _compatible_, but not equal
        * resolution rules are in the Avro specification
        * adding or removing only of fields with default values

        --

        ### How does Data and Schema find together?

        1. You include it in the Payload (as in the Sample)
        1. You versionize it, store it yourself and join it later (typical Database Application)
        1. Roll your Schema Registry and do Avro RPC

        --

        ### What benefits to we get from that?

        1. Support for dynamically generated schemas (Protobuf and Thrift want to be hand-crafted)
        1. Simpler interoperability with dynamically typed languages

        ### And the downsides

        1. Protocol Overhead or Runtime Dependency, pick a poison
        1. Need to verify schema changes are compatible

    </script>
</section>

<section data-markdown data-separator="^\n---\n$" data-separator-vertical="^\n--\n$">
    <script type="text/template">
        ## Kryo

        --

        ### The 'odd' one of our little list

        * Object Graph serialization Framework for the JVM

        * No Schema evolution

        * Blazing fast JVM-to-JVM communication

        * good candidate for live cluster communication (Akka e.g.) 

        [Source](https://github.com/EsotericSoftware/kryo)

        --

        ### Usage with SBT

        using twitter chill

        * [chill](https://github.com/twitter/chill)

        (live Sample)

        --

        ### Let's check the encoding on Disk


        16 bytes, maximum density

        |field| type(?) |value (last byte starts with 1)| actual|
        |--|--|--|--|
        |01|01|4368 72697374 6961EE|Christian|
        |02|01|5568 EC|Uhl|

        --

        ### Kryo Summary

        * no Schema

        * therefore no Schema Evolution

        * Good for Message-Serialization for running instances

        * you do not want to persist these serialized Values
    	
    </script>
</section>

<section data-markdown data-separator="^\n---\n$" data-separator-vertical="^\n--\n$">
    <script type="text/template">
        ## Summary

        --

        ## Summary

        |Framework  |Notes|
        |--|--|
        |Protobuf|nice integration, clear concepts,confusing 2->3 version change |
        |Thrift|clear concepts,bigger framework, lacking integration| 
        |Avro|good integration, good for big files,embedded schema concept| 
        |kryo|speed, simplicity,no evolvability, not for persisting|  

        --

        ## Relevant Books

        * [Effective Java](https://www.amazon.de/Effective-Java-Joshua-Bloch/dp/0134685997)
        * [Desinging Data Intensive Applications](https://www.amazon.de/Designing-Data-Intensive-Applications-Reliable-Maintainable/dp/1449373321)

        --

        ## Questions?

        My twitter is [@chrisuhl](https://twitter.com/chrisuhl)
    </script>
</section>

			</div>
		</div>

		<script src="lib/js/head.min.js"></script>
		<script src="js/reveal.js"></script>

		<script>
			// More info about config & dependencies:
			// - https://github.com/hakimel/reveal.js#configuration
			// - https://github.com/hakimel/reveal.js#dependencies
			Reveal.initialize({
				dependencies: [
					{ src: 'plugin/markdown/marked.js' },
					{ src: 'plugin/markdown/markdown.js' },
					{ src: 'plugin/notes/notes.js', async: true },
					{ src: 'plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } }
				]
			});
		</script>
	</body>
</html>
